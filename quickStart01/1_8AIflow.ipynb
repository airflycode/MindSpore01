{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI端到端\n",
    "\n",
    "Data - Model - Optimize - Save - Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n",
      "Namespace(device_target='CPU')\n"
     ]
    }
   ],
   "source": [
    "import mindspore  #导入mindspore AI框架\n",
    "print(mindspore.__version__)  #打印mindspore版本号\n",
    "import os \n",
    "# import moxing as mox #导入moxing网络模型开发API模块并简名为mox\n",
    "import argparse # 参数-解析\n",
    "from mindspore import context # 上下文 #导入mindspore子模块context，其下有set_context方法来配置运行需要的信息\n",
    "parser = argparse.ArgumentParser(description=\"MindSpore LeNet Example\")\n",
    "parser.add_argument(\"--device_target\" ,type=str ,default=\"CPU\",choices=[\"Ascend\",\"GPU\",\"CPU\"])\n",
    "# print(parser)\n",
    "\n",
    "args = parser.parse_known_args()[0] # 需要训练\n",
    "print(args)\n",
    "context.set_context(mode = context.GRAPH_MODE,device_target = args.device_target) #通过set_context方法配置，mode设置运行模式(动态图模式)，用CPU运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行使用图模式\n",
    "根据需求选择运行设备\n",
    "\n",
    "运行如下处理以获取数据集（建议使用mobaxterm打开一个terminal,jupyter不能运行）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "mkdir -p ./datasets/MNIST_Data/train ./datasets/MNIST_Data/test\n",
    "wget -NP ./datasets/MNIST_Data/train https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte\n",
    "wget -NP ./datasets/MNIST_Data/train https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\n",
    "wget -NP ./datasets/MNIST_Data/test https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\n",
    "wget -NP ./datasets/MNIST_Data/test https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte\n",
    "tree ./datasets/MNIST_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "mindspore.dataset中定义了许多方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds  # 导入mindspore子数据集模块，这个API模块用于数据处理主，存储样本和标签，同时也集成了一些常见的数据处理方法。\n",
    "import mindspore.dataset.transforms.c_transforms as C # 导入MindSpore提供的支持常见的图形增强功能的模块对图像进行预处理，简名为C\n",
    "import mindspore.dataset.vision.c_transforms as CV # 导入MindSpore提供的增强数据集模块对图像进行预处理，从而提高模型的广泛性，简名为CV\n",
    "from mindspore.dataset.vision import Inter # 导入MindSpore提供的vision子模块可以调整图像大小\n",
    "from mindspore import dtype as mstype  # 导入数据类型转换处理模块 定义为mindspore的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的处理分为四个阶段：\n",
    " - 定义`create_dataset`来创建数据集\n",
    " - 定义需要进行的数据增强和处理操作 为之后map映射做准备\n",
    " - 使用map映射函数，将数据操作应用到数据集\n",
    " - 进行数据shuffle batch等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_path,batch_size = 32, repeat_size = 1, num_parallel_workers = 1):\n",
    "    # 定义数据集\n",
    "    # mnist_ds = ds.MindDataset(data_path) #定义数据集\n",
    "    mnist_ds = ds.MnistDataset(data_path) #定义数据集\n",
    "    resize_height,resize_width = 32,32 #设置图像变量高与宽\n",
    "    \n",
    "    # 无偏移的标准化操作参数\n",
    "    rescale = 1.0/255.0\n",
    "    shift = 0.0 \n",
    "    \n",
    "    # 带偏移的标准化操作参数\n",
    "    rescale_nml = 1/0.3801\n",
    "    shift_nml = -1*0.1307/0.3081\n",
    "    \n",
    "    # * 定义需要进行的数据增强和处理操作，为之后进行map映射做准备\n",
    "    \n",
    "    resize_op = CV.Resize((resize_height,resize_width),interpolation=Inter.LINEAR) #缩小或者放大函数至预定高与宽，指定像素插值方式为双线性插值\n",
    "    \n",
    "    # 带偏移的标准化、归一化操作\n",
    "    rescale_nml_op = CV.Rescale(rescale_nml,shift_nml) # with: output = image * rescale + shift.    \n",
    "    # 对图像数据进行标准化、归一化操作，使得每个像素的数值大小在（0,1）范围中，可以提升训练效率，shift增加偏移量\n",
    "    rescale_op = CV.Rescale(rescale ,shift) \n",
    "    hwc2chw_op = CV.HWC2CHW()# 对图像数据张量进行变换，张量形式由高x宽x通道（HWC）变为通道x高x宽（CHW），方便进行数据训练。\n",
    "    type_cast_op = C.TypeCast(mstype.int32)#  将数据类型转化为int32\n",
    "    \n",
    "    # * 使用map映射函数，将数据操作应用到数据集\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op,input_columns=\"label\",num_parallel_workers=num_parallel_workers)  #应用到数据集操作方法 数据类型转换，操作列为标签列，并行一接口计算\n",
    "    mnist_ds = mnist_ds.map(operations=resize_op,input_columns=\"image\",num_parallel_workers=num_parallel_workers)     #应用到数据集操作方法 对图像放大缩小处理，预备插值，操作列为图像列，并行一接口计算\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_op,input_columns=\"image\",num_parallel_workers=num_parallel_workers)    #应用到数据集操作方法 编译文件标准化归一化，操作列为图象列，并行一接口计算\n",
    "    mnist_ds = mnist_ds.map(operations=rescale_nml_op,input_columns=\"image\",num_parallel_workers=num_parallel_workers)#应用到数据集操作方法 编译文件标准化归一化，操作列为图象列，并行一接口计算\n",
    "    mnist_ds = mnist_ds.map(operations=hwc2chw_op,input_columns=\"image\",num_parallel_workers=num_parallel_workers)    #应用到数据集操作方法 张量进行变换，操作列为图象列，并行一接口计算\n",
    "    \n",
    "    # * 进行shuffle、batch操作\n",
    "    buffer_size = 10000\n",
    "    mnist_ds=mnist_ds.shuffle(buffer_size=buffer_size) #先进行shuffle、batch操作，再进行repeat操作，这样能保证1个epoch内数据不重复\n",
    "\n",
    "    # 将整个数据集按照batch_size的大小分为若干批次，每一次训练的时候都是按一个批次的数据进行训练，\n",
    "    # drop_remainder确定是否删除数据行数小于批大小的最后一个块，这里设置为True就是只保留数据集个数整除\n",
    "    mnist_ds=mnist_ds.batch(batch_size=batch_size,drop_remainder=True) \n",
    "    \n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)  #将数据集重复repeat_size次，注意该操作一般使用在batch操作之后\n",
    "    \n",
    "    return mnist_ds  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mindspore.dataset.engine.datasets.RepeatDataset at 0x22b520618c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset(data_path= \"./datasets/MNIST_Data/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型\n",
    "使用mindspore创建模型需要继承`mindspore.nn.Cell` Cell是(covn2d-relu-softmax)等的基类\n",
    "\n",
    "网络的各层在`__init__`中定义，然后通过定义`construct`方法来完成模型的前向构造\n",
    "\n",
    "![](1651281990000.png)\n",
    "\n",
    "根据LeNet的网络结构 定义网络各层如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn  # 创建模型，配置网络结构（卷积，压平，全连接，激活函数，最大池化），导入nn子模块简名为nn\n",
    "from mindspore.common.initializer import Normal # 导入Normal模块类配置初始权重（正态分布）\n",
    "\n",
    "class LeNet5(nn.Cell):  #定义LeNet5类并继承nn.Cell类的属性和方法\n",
    "    def __init__(self,num_class = 10,num_channel = 1):\n",
    "        super(LeNet5,self).__init__()   #声明继承父类nn.cell的__init__方法\n",
    "        #定义需要的运算\n",
    "        \n",
    "        # * nn.Conv2d的第一个参数是输入图片的通道数为1，即单个过滤器应有的通道数，\n",
    "        # 第二个参数是输出图片的通道数，第三个参数是过滤器的二维属性，\n",
    "        # 它可以是一个int元组，但由于一般过滤器都是a * a形式的，而且为奇数。所以这里填入单个数即可\n",
    "        # 参数pad_mode为卷积方式，valid卷积即padding为0的卷积\n",
    "        # 现在也比较流行same卷积，即卷积后输出的图片不会缩小。\n",
    "        # 需要注意的是卷积层我们是不需要设置参数的随机方式的，因为它默认会给我们选择为Noremal\n",
    "        self.conv1 = nn.Conv2d(num_channel,6,5,pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6,16,5,pad_mode='valid')\n",
    "        \n",
    "        # * nn.Dense为致密连接层\n",
    "        # 它的第一个参数为输入层的维度，第二个参数为输出的维度，\n",
    "        # 第三个参数为神经网络可训练参数W权重矩阵的初始化方式，默认为normal，以上代码对号入座即可\n",
    "        self.fc1 = nn.Dense(16*5*5,120,weight_init = Normal(0.02))\n",
    "        self.fc2 = nn.Dense(120,84,weight_init = Normal(0.02))\n",
    "        self.fc3 = nn.Dense(84,num_class,weight_init = Normal(0.02))\n",
    "        self.relu = nn.ReLU()   # * nn.ReLU()非线性激活函数，它往往比论文中的sigmoid激活函数具有更好的效益\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2,stride=2)  # * nn.MaxPool2d为最大池化层的定义，kernel_size为采样器的大小，stride为采样步长，本例中将其都设置为2相当于将图片的宽度和高度都缩小一半\n",
    "        self.flatten = nn.Flatten()  # * nn.Flatten为输入展成平图层，即去掉那些空的维度\n",
    "        \n",
    "    def construct(self,x):\n",
    "        # 构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)      \n",
    "        return x\n",
    "    \n",
    "net = LeNet5()\n",
    "# print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义超参\n",
    "lr = learning_rate 学习率\n",
    "\n",
    "momentum = [动量方法避免局部最优化](https://blog.csdn.net/weixin_43687366/article/details/108214854)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 #learingrate,学习率，可以使梯度下降的幅度变小，从而可以更好的训练参数\n",
    "momentum = 0.9\n",
    "epoch_size = 1 # 每个epoch(代、时代)需要遍历完成图片的bacth(批次、批量)数一次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模型参数\n",
    "要训练神经网络模型，需要你定义损失函数和优化器\n",
    "\n",
    "MindSpore支持的\n",
    "- 损失函数  => [常见](https://zhuanlan.zhihu.com/p/401010037)\n",
    "  - [`SoftMaxCrossEntropyWithLogits`](https://zhuanlan.zhihu.com/p/51431626)\n",
    "  - `L1Loss`\n",
    "  - `MSELoss`\n",
    "- 优化器\n",
    "  - `Adam`\n",
    "  - `AdamWeightDecay`\n",
    "  - `Momentum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数：又叫目标函数，用于衡量预测值与实际值差异的程度。\n",
    "# 深度学习通过不停地迭代来缩小损失函数的值。\n",
    "# 定义一个好的损失函数，可以有效提高模型的性能。\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True,reduction='mean') # 损失函数 \n",
    "#相当于softmax分类器\n",
    "#sparse指定标签（label）是否使用稀疏模式，默认为false,reduction为损失的减少类型：mean表示平均值，一般\n",
    "#情况下都是选择平均地减少\n",
    "\n",
    "#优化器\n",
    "net_opt = nn.Momentum(net.trainable_params(),learning_rate=lr,momentum=0.9)  # 其中grad、lr、p、v和u分别表示梯度、学习率、参数、力矩和动量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练及保存模型\n",
    "mindspore的callback 可以在训练的时候执行自定义逻辑\n",
    "\n",
    "使用`ModelCheckPoint`为例\n",
    "\n",
    "可以保存网络模型和参数\n",
    "\n",
    "以便后续进行 Fine-tuning (微调) 操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint,CheckpointConfig\n",
    "# 设置模型保存参数\n",
    "model_save_path = \"./model/LeNet5/ckpt/\"\n",
    "\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=125,keep_checkpoint_max=10)\n",
    "# 应用模型保存参数\n",
    "ckpoint = ModelCheckpoint(prefix=\"lenet5\", directory = model_save_path, config=config_ck)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用 mindspore 提供的`model.train` 接口可以方便地进行网络的训练\n",
    "\n",
    "`LossMonitor`可以监控训练过程中`loss`值的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.nn import Accuracy\n",
    "from mindspore.train.callback import LossMonitor\n",
    "from mindspore import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(args, model:Model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
    "    \"\"\"定义训练的方法\"\"\"\n",
    "    # DATA_DIR = \"./datasets/MNIST_Data/train\"\n",
    "    ds_train = create_dataset(os.path.join(data_path,\"train\"), 32, repeat_size)\n",
    "    # ds_train = create_dataset(DATA_DIR, 32, repeat_size)\n",
    "    \n",
    "    # 调用Model类的train方法进行训练，LossMonitor(125)每隔125个step打印训练过程中的loss值 \n",
    "    # dataset_sink_mode为设置数据下沉模式，但该模式不支持CPU，所以这里我们只能设置为False\n",
    "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb,LossMonitor(125)],dataset_sink_mode = sink_mode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 dataset_sink_mode 用于控制数据是否下沉\n",
    "\n",
    "数据下沉是指数据通过通道直接传送Device上 可以加快训练速度 True表示下沉 否则非下沉（非Ascend设备好像不能下沉）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过模型运行测试数据接得到的结果可以验证模型的泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(network:nn.Cell, model:Model, data_path):\n",
    "    DATA_TEST = \"./datasets/MNIST_Data/test\"\n",
    "    ds_eval = create_dataset(os.path.join(data_path,\"test\"))\n",
    "    # ds_eval = create_dataset(DATA_TEST)\n",
    "    acc = model.eval(ds_eval,dataset_sink_mode=False)\n",
    "    print(\"{}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行流程\n",
    "设置使用的文件名、训练控制变量\n",
    "\n",
    "`train_epoch`设置为1 进行一个迭代的训练\n",
    "\n",
    "在`train_net`和`test_net`方法中我们加载之前下载的数据集\n",
    "\n",
    "`data_path` 是 MNIST 数据集的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 125, loss is 2.303574800491333\n",
      "epoch: 1 step: 250, loss is 2.2980284690856934\n",
      "epoch: 1 step: 375, loss is 2.3059816360473633\n",
      "epoch: 1 step: 500, loss is 2.3096466064453125\n",
      "epoch: 1 step: 625, loss is 2.309044361114502\n",
      "epoch: 1 step: 750, loss is 2.2958693504333496\n",
      "epoch: 1 step: 875, loss is 1.689231038093567\n",
      "epoch: 1 step: 1000, loss is 0.7370644211769104\n",
      "epoch: 1 step: 1125, loss is 0.37256941199302673\n",
      "epoch: 1 step: 1250, loss is 0.20879177749156952\n",
      "epoch: 1 step: 1375, loss is 0.12545432150363922\n",
      "epoch: 1 step: 1500, loss is 0.14776207506656647\n",
      "epoch: 1 step: 1625, loss is 0.2144518941640854\n",
      "epoch: 1 step: 1750, loss is 0.037226323038339615\n",
      "epoch: 1 step: 1875, loss is 0.10260177403688431\n",
      "{'Accuary': 0.9489182692307693}\n"
     ]
    }
   ],
   "source": [
    "train_epoch = 1\n",
    "# 路径\n",
    "data_path = \"./datasets/MNIST_Data\"\n",
    "\n",
    "# os.system('rm -f {}*.ckpt {}*.meta {}*.pb'.format(model_path, model_path, model_path)) # 移除先前的ckpt等files linux下是这样\n",
    "# os.remove(model_save_path[:-1]) # 移除先前的ckpt等files jupyter 实现不了\n",
    "\n",
    "dataset_size = 1\n",
    "\n",
    "model = Model(net, net_loss, net_opt, metrics={\"Accuary\":Accuracy()})\n",
    "# 训练网络\n",
    "train_net(args=args, model=model, epoch_size=train_epoch, data_path=data_path, repeat_size=dataset_size, ckpoint_cb=ckpoint,sink_mode=False)\n",
    "\n",
    "# 验证网络\n",
    "test_net(network=net, model=model, data_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "# 加载已经保存的模型\n",
    "param_dict = load_checkpoint(\"./model/LeNet5/ckpt/lenet5_1-1_1750.ckpt\")\n",
    "# 加载参数到网络中\n",
    "load_param_into_net(net=net, parameter_dict= param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证模型\n",
    "使用生成的模型进行单个图片的分类于此，具体步骤如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载验证集并创建可迭代对象 data就包含了一组（6个）用于测试的图像\n",
    "ds_test = create_dataset(os.path.join(data_path, \"test\"),batch_size=6).create_dict_iterator()\n",
    "data = next(ds_test)\n",
    "\n",
    "# images 为测试图片 ，label为实际分类\n",
    "images = data[\"image\"].asnumpy()\n",
    "label = data[\"label\"].asnumpy()\n",
    "plt.figure()\n",
    "for i in range(1,7):\n",
    "    plt.subplot(2,3,i)\n",
    "    plt.imshow(images[i-1][0], interpolation='None', cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# 使用函数 model.predict 预测对应的分类\n",
    "output = model.predict(Tensor(data['image']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e203e762bfffed2ed95f356b7746f45c38e9111b20c3e823e88c83c42bbc40aa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindsp161')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
