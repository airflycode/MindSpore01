{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动微分\n",
    "训练神经网络时候最常用的算法是 反向传播\n",
    "\n",
    "算法根据损失函数对于给定参数的梯度来调整参数（模型的权重）\n",
    "\n",
    "mindspore 计算一阶导数的方法是\n",
    "\n",
    " `mindspore.ops.GradOperation(get_all=False, get_by_list=False, sens_param=False)`\n",
    " \n",
    "get_all=False 只会对第一个输入求导 True对所有输入求导\n",
    "get_list=False 不会对权重求导 True 对权重求导\n",
    "sens_param 对网络的输出值做缩放以改变最终梯度 \n",
    "\n",
    "对 MatMul 算子的求导做深入分析\n",
    "\n",
    "导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "from mindspore import ParameterTuple, Parameter\n",
    "from mindspore import dtype as mstype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对输入求一阶导\n",
    "首先需要定义一个需要求导的网络，以一个MatMul算子构成的网络f(x,y) = z*x*y为例\n",
    "网络如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.matmul = ops.MatMul()\n",
    "        # 在这里就可以指定使用的类型是mindspore.float32或者别的mstype类\n",
    "        self.z = Parameter(Tensor(np.array([1.0]),mstype.float32),name = 'z') \n",
    "        # print(type(self.z))\n",
    "    \n",
    "    def construct(self,x,y):\n",
    "        x = x*self.z\n",
    "        out = self.matmul(x,y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 定义求导网络\n",
    "\n",
    "__init__定义需要求到的网络self.net 和 ops.GradOperation操作\n",
    "\n",
    "construct 对self.net进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradNetWrtY(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNetWrtY,self).__init__()\n",
    "        self.net = net\n",
    "        self.grad_op = ops.GradOperation()\n",
    "        self.grad_op1 = ops.GradOperation(get_all=True)\n",
    "\n",
    "    def construct(self,x,y):\n",
    "        gradient_function = self.grad_op1(self.net)\n",
    "        return gradient_function(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义输入并且打印输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mindspore.common.parameter.ParameterTensor'>\n",
      "(Tensor(shape=[2, 3], dtype=Float32, value=\n",
      "[[4.50999975e+000, 2.70000005e+000, 3.60000014e+000],\n",
      " [4.50999975e+000, 2.70000005e+000, 3.60000014e+000]]), Tensor(shape=[3, 3], dtype=Float32, value=\n",
      "[[2.59999990e+000, 2.59999990e+000, 2.59999990e+000],\n",
      " [1.89999998e+000, 1.89999998e+000, 1.89999998e+000],\n",
      " [1.30000007e+000, 1.30000007e+000, 1.30000007e+000]]))\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([[0.8,0.6,0.2],[1.8,1.3,1.1]],dtype=mstype.float32)\n",
    "y = Tensor([[0.11,3.3,1.1],[1.1,0.2,1.4],[1.1,2.2,0.3]],dtype=mstype.float32)\n",
    "output = GradNetWrtY(Net()).construct(x,y)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对权重求一阶导\n",
    "将 ops.GradOperation 中的 get_by_list 设置为 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mindspore.common.parameter.ParameterTensor'>\n",
      "(Tensor(shape=[1], dtype=Float32, value= [2.15359993e+001]),)\n"
     ]
    }
   ],
   "source": [
    "class GradNetWrtX(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNetWrtX,self).__init__()\n",
    "        self.net = net\n",
    "        self.param = ParameterTuple(net.trainable_params())\n",
    "        self.grad_op = ops.GradOperation(get_by_list=True)\n",
    "\n",
    "    def construct(self,x,y):\n",
    "        gradient_function = self.grad_op(self.net,self.param)\n",
    "        return gradient_function(x,y)\n",
    "    \n",
    "output = GradNetWrtX(Net()).construct(x,y)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要某些权重不求导？\n",
    "\n",
    "在定义求导网络的时候，对应的权重中 `requires_grad`设置为False\n",
    "```python\n",
    "self.z = Parameter(Tensor(np.array([1.0])),mstype.float32,name = 'z',required_grad = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度值缩放\n",
    "可以通过`sens_param` 参数对网络的输出值做缩放以改变最终梯度\n",
    "\n",
    "首先 `ops.GradOperation`的`sens_param`设置为True\n",
    "\n",
    "缩放指数 `self.grad_wrt_output` 可以这样记录\n",
    "```python\n",
    "    self.grad_wrt_output = Tensor([s1,s2,s3],[s4,s5,s6])\n",
    "```\n",
    "则GradNetWrtZ的结构为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mindspore.common.parameter.ParameterTensor'>\n",
      "[[9.0199995 5.4       7.2000003]\n",
      " [4.5099998 2.7       3.6000001]]\n"
     ]
    }
   ],
   "source": [
    "class GradNetWrtZ(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNetWrtZ,self).__init__()\n",
    "        self.net = net\n",
    "        self.grad_op = ops.GradOperation(sens_param=True)\n",
    "        self.grad_wrt_output = Tensor(np.array(([[2,2,2],[1,1,1]])),dtype=mstype.float32)\n",
    "        # 缩放指数按照上述设置，最后的结果中 第一列都是第二列的两倍 这就是缩放指数的作用\n",
    "\n",
    "    def construct(self,x,y):\n",
    "        gradient_function = self.grad_op(self.net)\n",
    "        return gradient_function(x,y,self.grad_wrt_output)\n",
    "    \n",
    "output = GradNetWrtZ(Net()).construct(x,y)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停止计算梯度\n",
    "可以使用stop_gradient来禁止网络内算子对梯度的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.0199995 5.4       7.2000003]\n",
      " [9.0199995 5.4       7.2000003]]\n"
     ]
    }
   ],
   "source": [
    "from mindspore.ops.functional import stop_gradient\n",
    "class Net1(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net1,self).__init__()\n",
    "        self.matmul = ops.MatMul()\n",
    "    def construct(self,x,y):\n",
    "        out1 = self.matmul(x,y)\n",
    "        out2 = self.matmul(x,y)\n",
    "        # out2 = stop_gradient(out2) \n",
    "        out = out1+out2\n",
    "        return out       \n",
    "    \n",
    "class GradNetWrtO(nn.Cell):\n",
    "    def __init__(self,net):\n",
    "        super(GradNetWrtO,self).__init__()\n",
    "        self.net = net\n",
    "        self.grad_op = ops.GradOperation()\n",
    "    \n",
    "    def construct(self,x,y):\n",
    "        gradient_function = self.grad_op(self.net)\n",
    "        return gradient_function(x,y)\n",
    "    \n",
    "output = GradNetWrtO(Net1())(x,y)\n",
    "print(output)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e203e762bfffed2ed95f356b7746f45c38e9111b20c3e823e88c83c42bbc40aa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindsp161')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
