{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模型参数\n",
    "### 超参\n",
    "\n",
    "是可以调整的参数，可以 控制 模型训练优化 的 过程\n",
    "\n",
    "不同的超参可能会影响 模型训练 和 收敛速度\n",
    "\n",
    "一般会定义以下用于训练的超参：\n",
    " - 训练轮次（epoch）：训练是遍历数据集的次数\n",
    " - 批次大小（batch size）：数据集进行分批读取训练，设定每个批次数据的大小\n",
    " - 学习率（learning rate）：学习率偏小 可能收敛过慢  学习率过大有可能会不收敛或者不可预测的结果\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数用来评价模型的**预测值**与**真实值**偏离的程度\n",
    "\n",
    "使用绝对误差损失函数L1Loss\n",
    "\n",
    "还有很多Loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "output_data = Tensor(np.array([[1,2,3],[2,3,4]]),dtype = mstype.float32)\n",
    "target_data = Tensor(np.array([[0,2,5],[3,1,1]]),dtype = mstype.float32)\n",
    "print(loss(output_data,target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "优化器用于 计算 和 更新 梯度\n",
    "\n",
    "模型优化算法的选择直接关系到最终模型的的性能\n",
    "\n",
    "效果不好，未必是特征和模型设计的问题，很有可能是优化器的问题\n",
    "\n",
    "MindSpore所有优化器都封装在`Optimizer`对象中 \n",
    "\n",
    "本次例子中我们使用SGD优化器\n",
    "\n",
    "另有其他很多优化器在`mindspore.nn.optim`中\n",
    "\n",
    "构建一个`Optimizer`对象，这个对象能够保持当前参数并给予计算得到的参数进行参数更新\n",
    "\n",
    "他需要一个可以优化需要优化的参数（Variable对象）的迭代器 例如网络中所有可以训练的`parameter`\n",
    "\n",
    "将`params`设置为`net.trainable_params()`即可\n",
    "\n",
    "然后设置Optimizer的参数选项 学习率 权重衰减等\n",
    "\n",
    "样例：\n",
    "```python\n",
    "from mindspore import nn\n",
    "opyim=nn.SGD(params = net.trainable_params(),learning_rate=0.1,weight_decay = 0.0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "一般有四个步骤\n",
    " - 定义神将网络\n",
    " - 构建数据集\n",
    " - 定义超参、损失函数、优化器\n",
    " - 输入训练轮次和数据集 进行训练\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e203e762bfffed2ed95f356b7746f45c38e9111b20c3e823e88c83c42bbc40aa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindsp161')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
