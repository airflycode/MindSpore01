{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模型参数\n",
    "### 超参\n",
    "\n",
    "是可以调整的参数，可以 控制 模型训练优化 的 过程\n",
    "\n",
    "不同的超参可能会影响 模型训练 和 收敛速度\n",
    "\n",
    "一般会定义以下用于训练的超参：\n",
    " - 训练轮次（epoch）：训练是遍历数据集的次数\n",
    " - 批次大小（batch size）：数据集进行分批读取训练，设定每个批次数据的大小\n",
    " - 学习率（learning rate）：学习率偏小 可能收敛过慢  学习率过大有可能会不收敛或者不可预测的结果\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数用来评价模型的**预测值**与**真实值**偏离的程度\n",
    "\n",
    "使用绝对误差损失函数L1Loss\n",
    "\n",
    "还有很多Loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "output_data = Tensor(np.array([[1,2,3],[2,3,4]]),dtype = mstype.float32)\n",
    "target_data = Tensor(np.array([[0,2,5],[3,1,1]]),dtype = mstype.float32)\n",
    "print(loss(output_data,target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "优化器用于 计算 和 更新 梯度\n",
    "\n",
    "模型优化算法的选择直接关系到最终模型的的性能\n",
    "\n",
    "效果不好，未必是特征和模型设计的问题，很有可能是优化器的问题\n",
    "\n",
    "MindSpore所有优化器都封装在`Optimizer`对象中 \n",
    "\n",
    "本次例子中我们使用SGD优化器\n",
    "\n",
    "另有其他很多优化器在`mindspore.nn.optim`中\n",
    "\n",
    "构建一个`Optimizer`对象，这个对象能够保持当前参数并给予计算得到的参数进行参数更新\n",
    "\n",
    "他需要一个可以优化需要优化的参数（Variable对象）的迭代器 例如网络中所有可以训练的`parameter`\n",
    "\n",
    "将`params`设置为`net.trainable_params()`即可\n",
    "\n",
    "然后设置Optimizer的参数选项 学习率 权重衰减等\n",
    "\n",
    "样例：\n",
    "```python\n",
    "from mindspore import nn\n",
    "opyim=nn.SGD(params = net.trainable_params(),learning_rate=0.1,weight_decay = 0.0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "一般有四个步骤\n",
    " - 定义神将网络\n",
    " - 构建数据集\n",
    " - 定义超参、损失函数、优化器\n",
    " - 输入训练轮次和数据集 进行训练\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv1.weight', Parameter (name=conv1.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True))\n",
      "('conv2.weight', Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True))\n",
      "('fc1.weight', Parameter (name=fc1.weight, shape=(120, 256), dtype=Float32, requires_grad=True))\n",
      "('fc1.bias', Parameter (name=fc1.bias, shape=(120,), dtype=Float32, requires_grad=True))\n",
      "('fc2.weight', Parameter (name=fc2.weight, shape=(84, 120), dtype=Float32, requires_grad=True))\n",
      "('fc2.bias', Parameter (name=fc2.bias, shape=(84,), dtype=Float32, requires_grad=True))\n",
      "('fc3.weight', Parameter (name=fc3.weight, shape=(10, 84), dtype=Float32, requires_grad=True))\n",
      "('fc3.bias', Parameter (name=fc3.bias, shape=(10,), dtype=Float32, requires_grad=True))\n",
      "\n",
      "\n",
      "Parameter (name=conv1.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc1.weight, shape=(120, 256), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc1.bias, shape=(120,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc2.weight, shape=(84, 120), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc2.bias, shape=(84,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc3.weight, shape=(10, 84), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc3.bias, shape=(10,), dtype=Float32, requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(3696:20140,MainProcess):2022-04-30-18:39:43.665.166 [mindspore\\train\\model.py:536] The CPU cannot support dataset sink mode currently.So the training process will be performed with dataset not sink.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 2.302577257156372\n",
      "epoch: 1 step: 2, loss is 2.3023529052734375\n",
      "epoch: 2 step: 1, loss is 2.3015079498291016\n",
      "epoch: 2 step: 2, loss is 2.299830198287964\n",
      "epoch: 3 step: 1, loss is 2.3004965782165527\n",
      "epoch: 3 step: 2, loss is 2.297396421432495\n",
      "epoch: 4 step: 1, loss is 2.299537181854248\n",
      "epoch: 4 step: 2, loss is 2.2950453758239746\n",
      "epoch: 5 step: 1, loss is 2.298628807067871\n",
      "epoch: 5 step: 2, loss is 2.2927725315093994\n",
      "epoch: 6 step: 1, loss is 2.29776930809021\n",
      "epoch: 6 step: 2, loss is 2.290576934814453\n",
      "epoch: 7 step: 1, loss is 2.2969565391540527\n",
      "epoch: 7 step: 2, loss is 2.28845477104187\n",
      "epoch: 8 step: 1, loss is 2.296186923980713\n",
      "epoch: 8 step: 2, loss is 2.286400556564331\n",
      "epoch: 9 step: 1, loss is 2.2954583168029785\n",
      "epoch: 9 step: 2, loss is 2.2844114303588867\n",
      "epoch: 10 step: 1, loss is 2.2947683334350586\n",
      "epoch: 10 step: 2, loss is 2.282487154006958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor,Model\n",
    "from mindspore import  dtype as mstype\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
    "parser.add_argument('--device_target', type=str, default=\"CPU\", choices=['Ascend', 'GPU', 'CPU'])\n",
    "\n",
    "\n",
    "DATA_DIR = \"./datasets/MNIST_Data/train\"\n",
    "# DATA_DIR = \"./datasets/cifar-10-batches-bin/train\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "#采样器\n",
    "sampler = ds.SequentialSampler(num_samples=5)\n",
    "\n",
    "# dataset = ds.Cifar100Dataset(DATA_DIR,sampler=sampler)\n",
    "\n",
    "dataset = ds.MnistDataset(DATA_DIR,sampler=sampler) #这个是MNIST数据集\n",
    "# dataset = ds.Cifar10Dataset(DATA_DIR,sampler=sampler) #这个是Cifar10数据集\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self,num_class=10,num_channel = 1):\n",
    "    # 初始化网络\n",
    "        super(LeNet5,self).__init__()\n",
    "        # 定义所需要的运算\n",
    "        self.conv1 = nn.Conv2d(num_channel,6,5,pad_mode='valid') # 卷积\n",
    "        self.conv2 = nn.Conv2d(6,16,5,pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(256,120) # 全连接层\n",
    "        # self.fc1 = nn.Dense(16*5*5,120) # 全连接层\n",
    "        self.fc2 = nn.Dense(120,84)\n",
    "        self.fc3 = nn.Dense(84,num_class)\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2,stride=2)# 最大池化-降采样\n",
    "        self.relu = nn.ReLU() # 激活函数\n",
    "        self.flatten = nn.Flatten()# flatten 扁平的意思=> 将原来的高维数组换成只有 一行 的数组 列数是之前的各维度之积\n",
    "\n",
    "    # 定义网络构建函数\n",
    "    def construct(self,x):\n",
    "        # 构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#初始化网路\n",
    "net = LeNet5()\n",
    "for m in net.parameters_and_names():\n",
    "    print(m)\n",
    "    \n",
    "#定义超参\n",
    "epoch = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    " \n",
    "#构建数据集\n",
    "sampler = ds.SequentialSampler(num_samples=128)\n",
    "dataset = ds.MnistDataset(DATA_DIR,sampler = sampler)\n",
    "\n",
    "#数据类型的转换\n",
    "type_cast_op_image = C.TypeCast(mstype.float32)\n",
    "type_cast_op_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "#数据序列读取方式\n",
    "HWC2CHW = CV.HWC2CHW()\n",
    "\n",
    "#构建数据集\n",
    "dataset = dataset.map(operations=[type_cast_op_image,HWC2CHW],input_columns=\"image\")\n",
    "dataset = dataset.map(operations=type_cast_op_label,input_columns=\"label\")\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "print(\"\\n\")\n",
    "#传入定义的超参\n",
    "for p in net.trainable_params():\n",
    "    print(p)\n",
    "\n",
    "optim = nn.SGD(params=net.trainable_params(),learning_rate=learning_rate)# 自动微分反向传播\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True,reduction='mean') # 交叉熵损失函数\n",
    "\n",
    "#开始训练：输入训练轮次和训练数据集\n",
    "model = Model(net,loss_fn=loss,optimizer=optim)\n",
    "model.train(epoch=epoch,train_dataset=dataset,callbacks=LossMonitor())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e203e762bfffed2ed95f356b7746f45c38e9111b20c3e823e88c83c42bbc40aa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindsp161')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
