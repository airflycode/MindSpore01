{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模型参数\n",
    "### 超参\n",
    "\n",
    "是可以调整的参数，可以 控制 模型训练优化 的 过程\n",
    "\n",
    "不同的超参可能会影响 模型训练 和 收敛速度\n",
    "\n",
    "一般会定义以下用于训练的超参：\n",
    " - 训练轮次（epoch）：训练是遍历数据集的次数\n",
    " - 批次大小（batch size）：数据集进行分批读取训练，设定每个批次数据的大小\n",
    " - 学习率（learning rate）：学习率偏小 可能收敛过慢  学习率过大有可能会不收敛或者不可预测的结果\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数用来评价模型的**预测值**与**真实值**偏离的程度\n",
    "\n",
    "使用绝对误差损失函数L1Loss\n",
    "\n",
    "还有很多Loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import dtype as mstype\n",
    "\n",
    "loss = nn.L1Loss()\n",
    "output_data = Tensor(np.array([[1,2,3],[2,3,4]]),dtype = mstype.float32)\n",
    "target_data = Tensor(np.array([[0,2,5],[3,1,1]]),dtype = mstype.float32)\n",
    "print(loss(output_data,target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "优化器用于 计算 和 更新 梯度\n",
    "\n",
    "模型优化算法的选择直接关系到最终模型的的性能\n",
    "\n",
    "效果不好，未必是特征和模型设计的问题，很有可能是优化器的问题\n",
    "\n",
    "MindSpore所有优化器都封装在`Optimizer`对象中 \n",
    "\n",
    "本次例子中我们使用SGD优化器\n",
    "\n",
    "另有其他很多优化器在`mindspore.nn.optim`中\n",
    "\n",
    "构建一个`Optimizer`对象，这个对象能够保持当前参数并给予计算得到的参数进行参数更新\n",
    "\n",
    "他需要一个可以优化需要优化的参数（Variable对象）的迭代器 例如网络中所有可以训练的`parameter`\n",
    "\n",
    "将`params`设置为`net.trainable_params()`即可\n",
    "\n",
    "然后设置Optimizer的参数选项 学习率 权重衰减等\n",
    "\n",
    "样例：\n",
    "```python\n",
    "from mindspore import nn\n",
    "opyim=nn.SGD(params = net.trainable_params(),learning_rate=0.1,weight_decay = 0.0)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "一般有四个步骤\n",
    " - 定义神将网络\n",
    " - 构建数据集\n",
    " - 定义超参、损失函数、优化器\n",
    " - 输入训练轮次和数据集 进行训练\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor,Model\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "DATA_DIR = \"./datasets/MNIST_Data/train\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "#采样器\n",
    "sampler = ds.SequentialSampler(num_samples=5)\n",
    "\n",
    "# dataset = ds.Cifar100Dataset(DATA_DIR,sampler=sampler)\n",
    "\n",
    "dataset = ds.MnistDataset(DATA_DIR,sampler=sampler) #这个是MNIST数据集\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self,num_class=10,num_channel = 1):\n",
    "    # 初始化网络\n",
    "        super(LeNet5,self).__init__()\n",
    "        # 定义所需要的运算\n",
    "        self.conv1 = nn.Conv2d(num_channel,6,5,pad_mode='valid') # 卷积\n",
    "        self.conv2 = nn.Conv2d(6,16,5,pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(16*5*5,120) # 全连接层\n",
    "        self.fc2 = nn.Dense(120,84)\n",
    "        self.fc3 = nn.Dense(84,num_class)\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2,stride=2)# 最大池化-降采样\n",
    "        self.relu = nn.ReLU() # 激活函数\n",
    "        self.flatten = nn.Flatten()# flatten 扁平的意思=> 将原来的高维数组换成只有 一行 的数组 列数是之前的各维度之积\n",
    "\n",
    "    # 定义网络构建函数\n",
    "    def construct(self,x):\n",
    "        # 构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "\n",
    "net = LeNet5()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e203e762bfffed2ed95f356b7746f45c38e9111b20c3e823e88c83c42bbc40aa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindsp161')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
