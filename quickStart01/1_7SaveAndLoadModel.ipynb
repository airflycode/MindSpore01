{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leNet 5 模型\n",
    "\n",
    "导入回调函数以保存网络生成文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一站式\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train import Model\n",
    "\n",
    "from mindvision.classification.dataset import Mnist\n",
    "from mindvision.classification.models import lenet\n",
    "from mindvision.engine.callback import LossMonitor\n",
    "\n",
    "epochs = 10  # 训练轮次\n",
    "\n",
    "# 1. 构建数据集\n",
    "# download_train = Mnist(path=\"./mnist\", split=\"train\", batch_size=32, repeat_num=1, shuffle=True, resize=32, download=True)\n",
    "# dataset_train = download_train.run()\n",
    "\n",
    "# 2. 定义神经网络\n",
    "network = lenet(num_classes=10, pretrained=False)\n",
    "# 3.1 定义损失函数\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "# 3.2 定义优化器函数\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)\n",
    "# 3.3 初始化模型参数\n",
    "model = Model(network, loss_fn=net_loss, optimizer=net_opt, metrics={'accuracy'})\n",
    "\n",
    "# 4. 对神经网络执行训练\n",
    "# model.train(epochs, dataset_train, callbacks=[LossMonitor(0.01, 1875)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv1.weight', Parameter (name=conv1.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True))\n",
      "('conv2.weight', Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True))\n",
      "('fc1.weight', Parameter (name=fc1.weight, shape=(120, 256), dtype=Float32, requires_grad=True))\n",
      "('fc1.bias', Parameter (name=fc1.bias, shape=(120,), dtype=Float32, requires_grad=True))\n",
      "('fc2.weight', Parameter (name=fc2.weight, shape=(84, 120), dtype=Float32, requires_grad=True))\n",
      "('fc2.bias', Parameter (name=fc2.bias, shape=(84,), dtype=Float32, requires_grad=True))\n",
      "('fc3.weight', Parameter (name=fc3.weight, shape=(10, 84), dtype=Float32, requires_grad=True))\n",
      "('fc3.bias', Parameter (name=fc3.bias, shape=(10,), dtype=Float32, requires_grad=True))\n",
      "\n",
      "\n",
      "Parameter (name=conv1.weight, shape=(6, 1, 5, 5), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=conv2.weight, shape=(16, 6, 5, 5), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc1.weight, shape=(120, 256), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc1.bias, shape=(120,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc2.weight, shape=(84, 120), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc2.bias, shape=(84,), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc3.weight, shape=(10, 84), dtype=Float32, requires_grad=True)\n",
      "Parameter (name=fc3.bias, shape=(10,), dtype=Float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 原生\n",
    "\n",
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor,Model\n",
    "from mindspore import  dtype as mstype\n",
    "from mindspore.train.callback import LossMonitor\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# 指定CPU训练\n",
    "parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
    "parser.add_argument('--device_target', type=str, default=\"CPU\", choices=['Ascend', 'GPU', 'CPU'])\n",
    "\n",
    "\n",
    "DATA_DIR = \"./datasets/MNIST_Data/train\"\n",
    "# DATA_DIR = \"./datasets/cifar-10-batches-bin/train\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "#采样器\n",
    "sampler = ds.SequentialSampler(num_samples=5)\n",
    "\n",
    "# dataset = ds.Cifar100Dataset(DATA_DIR,sampler=sampler)\n",
    "\n",
    "dataset = ds.MnistDataset(DATA_DIR,sampler=sampler) #这个是MNIST数据集\n",
    "# dataset = ds.Cifar10Dataset(DATA_DIR,sampler=sampler) #这个是Cifar10数据集\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self,num_class=10,num_channel = 1):\n",
    "    # 初始化网络\n",
    "        super(LeNet5,self).__init__()\n",
    "        # 定义所需要的运算\n",
    "        self.conv1 = nn.Conv2d(num_channel,6,5,pad_mode='valid') # 卷积\n",
    "        self.conv2 = nn.Conv2d(6,16,5,pad_mode='valid')\n",
    "        self.fc1 = nn.Dense(256,120) # 全连接层\n",
    "        # self.fc1 = nn.Dense(16*5*5,120) # 全连接层\n",
    "        self.fc2 = nn.Dense(120,84)\n",
    "        self.fc3 = nn.Dense(84,num_class)\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2,stride=2)# 最大池化-降采样\n",
    "        self.relu = nn.ReLU() # 激活函数\n",
    "        self.flatten = nn.Flatten()# flatten 扁平的意思=> 将原来的高维数组换成只有 一行 的数组 列数是之前的各维度之积\n",
    "\n",
    "    # 定义网络构建函数\n",
    "    def construct(self,x):\n",
    "        # 构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x    \n",
    "\n",
    "#初始化网路\n",
    "net = LeNet5()\n",
    "for m in net.parameters_and_names():\n",
    "    print(m)\n",
    "    \n",
    "#定义超参\n",
    "epoch = 12\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    " \n",
    "#构建数据集\n",
    "sampler = ds.SequentialSampler(num_samples=128)\n",
    "dataset = ds.MnistDataset(DATA_DIR,sampler = sampler)\n",
    "\n",
    "#数据类型的转换\n",
    "type_cast_op_image = C.TypeCast(mstype.float32)\n",
    "type_cast_op_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "#数据序列读取方式\n",
    "HWC2CHW = CV.HWC2CHW()\n",
    "\n",
    "#构建数据集\n",
    "dataset = dataset.map(operations=[type_cast_op_image,HWC2CHW],input_columns=\"image\")\n",
    "dataset = dataset.map(operations=type_cast_op_label,input_columns=\"label\")\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "print(\"\\n\")\n",
    "#传入定义的超参\n",
    "for p in net.trainable_params():\n",
    "    print(p)\n",
    "\n",
    "#函数\n",
    "optim = nn.SGD(params=net.trainable_params(),learning_rate=learning_rate)# 自动微分反向传播\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True,reduction='mean') # 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入callback函数进行模型的保存\n",
    "from mindspore.train.callback import ModelCheckpoint\n",
    "ckpt_cb = ModelCheckpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据需求对checkpoint函数进行配置\n",
    "from mindspore.train.callback import ModelCheckpoint,CheckpointConfig\n",
    "\n",
    "# save_checkpoint_steps=32 每32次保存一次\n",
    "# keep_checkpoint_max=10 最多保留10个checkpoint\n",
    "\n",
    "config_cb = CheckpointConfig(save_checkpoint_steps=4,keep_checkpoint_max=10)\n",
    "\n",
    "# prefix = 'lenet5' 定义checkpoint文件前缀名称\n",
    "# directory 指定保存的地址 \n",
    "\n",
    "ckpt_cb = ModelCheckpoint(prefix='lenet5',directory=\"../quickStart01/model/\",config=config_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6188:16984,MainProcess):2022-04-29-20:04:19.514.641 [mindspore\\train\\model.py:536] The CPU cannot support dataset sink mode currently.So the training process will be performed with dataset not sink.\n"
     ]
    }
   ],
   "source": [
    "#开始训练：输入训练轮次和训练数据集\n",
    "model = Model(net,loss_fn=loss,optimizer=optim)\n",
    "model.train(epoch=epoch,train_dataset=dataset,callbacks=ckpt_cb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1651218789000.png)\n",
    "\n",
    "其中  \n",
    "\n",
    "\\-graph.meta  是编译后的计算图\n",
    "\n",
    ".ckpt               是checkpoint文件，是权重文件\n",
    "\n",
    "![](1651218704000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型\n",
    "#### 进行 重训练 或者 推理\n",
    "要加载模型权重 需要 先创建 相同模型 的实例\n",
    "\n",
    "然后使用`load_checkpoint`和`load_param_into_net`方法加载参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建相同的模型实例\n",
    "from mindspore import load_checkpoint,load_param_into_net\n",
    "\n",
    "net0 = LeNet5()\n",
    "\n",
    "model0  = Model(net0,loss_fn = loss,metrics={\"accuracy\"})\n",
    "\n",
    "# 将模型参数存入字典\n",
    "param_dict = load_checkpoint(\"../quickStart01/model/lenet5_1-12_2.ckpt\")\n",
    "# print(param_dict)\n",
    "# 将参数加载到网络中去\n",
    "load_param_into_net(net0,param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证\n",
    "针对仅推理场景，把参数直接加载到网络中，以便后续的推理验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6188:16984,MainProcess):2022-04-29-20:04:20.883.610 [mindspore\\train\\model.py:954] CPU cannot support dataset sink mode currently.So the evaluating process will be performed with dataset non-sink mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.1171875}\n"
     ]
    }
   ],
   "source": [
    "# create_custom_dataset\n",
    "TEST_DIR = DATA_DIR[:-5]+\"test\"\n",
    "# print(TEST_DIR)\n",
    "# 定义验证数据集\n",
    "sampler = ds.SequentialSampler(num_samples=128)\n",
    "dataset_eval = ds.MnistDataset(TEST_DIR,sampler=sampler)\n",
    "# dataset_eval = create_dataset(TEST_DIR,32,1)\n",
    "\n",
    "#数据类型的转换\n",
    "type_cast_op_image = C.TypeCast(mstype.float32)\n",
    "type_cast_op_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "#数据序列读取方式\n",
    "HWC2CHW = CV.HWC2CHW()\n",
    "\n",
    "#构建数据集\n",
    "dataset_eval = dataset_eval.map(operations=[type_cast_op_image,HWC2CHW],input_columns=\"image\")\n",
    "dataset_eval = dataset_eval.map(operations=type_cast_op_label,input_columns=\"label\")\n",
    "dataset_eval = dataset_eval.batch(batch_size)\n",
    "\n",
    "# 调用eval()进行推理\n",
    "acc = model0.eval(dataset_eval)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迁移学习\n",
    "\n",
    "针对任务中断及微调（Fine-tuning）场景 可以加载网络参数和优化器参数到模型中。\n",
    "\n",
    "如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6188:16984,MainProcess):2022-04-29-20:11:09.738.293 [mindspore\\train\\model.py:536] The CPU cannot support dataset sink mode currently.So the training process will be performed with dataset not sink.\n"
     ]
    }
   ],
   "source": [
    "# 设置训练轮次\n",
    "\n",
    "epoch00 = 1\n",
    "\n",
    "# 定义训练数据集\n",
    "dataset00 = ds.MnistDataset(DATA_DIR,sampler = sampler) \n",
    "\n",
    "#数据类型的转换\n",
    "type_cast_op_image = C.TypeCast(mstype.float32)\n",
    "type_cast_op_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "#数据序列读取方式\n",
    "HWC2CHW = CV.HWC2CHW()\n",
    "\n",
    "#构建数据集\n",
    "dataset00 = dataset00.map(operations=[type_cast_op_image,HWC2CHW],input_columns=\"image\")\n",
    "dataset00 = dataset00.map(operations=type_cast_op_label,input_columns=\"label\")\n",
    "dataset00 = dataset00.batch(batch_size)\n",
    "\n",
    "# 调用train()进行训练\n",
    "model0.train(epoch00,dataset00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导出模型\n",
    "\n",
    "在模型训练过程中，可以添加检查点(CheckPoint)用于保存模型的参数，以便执行推理及再训练使用。\n",
    "\n",
    "如果想继续在不同硬件平台上做推理，可通过网络和CheckPoint格式文件生成对应的 MindIR(推荐，便于全场景使用)、AIR 或 ONNX 格式文件。\n",
    "\n",
    "以下通过示例来介绍保存CheckPoint格式文件和导出MindIR、AIR或ONNX格式文件的方法。\n",
    "\n",
    "### 导出MindIR格式\n",
    "\n",
    "当有了CheckPoint之后 如果想要跨平台或者硬件执行推理（昇腾AI、mindspore端侧、GPU等）\n",
    "\n",
    "可以通过定义网络结构和CheckPoint生成MINDIR格式模型文件\n",
    "\n",
    "当前支持基于静态图且不包含控制语义流的推理网络的导出\n",
    "\n",
    "例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import export,load_checkpoint,load_param_into_net\n",
    "from mindspore import Tensor\n",
    "import numpy as np\n",
    "lenet000 = LeNet5()\n",
    "# 将模型参数存入模型字典\n",
    "param_dict000 = load_checkpoint(\"../quickStart01/model/lenet5_1-10_2.ckpt\")\n",
    "\n",
    "# 将参数加载到网络中\n",
    "load_param_into_net(lenet000,param_dict000)\n",
    "\n",
    "# 这个input代表着网络的输入情况  如果有多个，多个都要传入\n",
    "# 这个size一定是 1 * 4 向量（四个维度）\n",
    "input000 = np.random.uniform(0.0,1.0,size = [32,1,28,28]).astype(np.float32)\n",
    "export(lenet000,Tensor(input000),file_name='outModel/lenet5_1-10_2',file_format=\"MINDIR\")\n",
    "# 多个传入\n",
    "# export(lenet000,Tensor(input000),Tensor(input001),file_name='lenet5_1-10_2',file_format=\"MINDIR\")\n",
    "# 导出的文件会以 .mindir 结尾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIR格式\n",
    "继续在昇腾AI处理器上推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mindspore\\ccsrc\\pipeline\\jit\\pipeline.cc:1720 ExportGraph] Only support export file in 'AIR' format with Ascend backend.\n\n# In file C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6188\\45943137.py(61)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6188\\2140194128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenet000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lenet5_1-10_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"AIR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 导出以 .air 结尾\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mindsp161\\lib\\site-packages\\mindspore\\train\\serialization.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(net, file_name, file_format, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    844\u001b[0m         \u001b[0m_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m         \u001b[0m_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mindsp161\\lib\\site-packages\\mindspore\\train\\serialization.py\u001b[0m in \u001b[0;36m_export\u001b[1;34m(net, file_name, file_format, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[0mreal_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m         \u001b[0m_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    879\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mS_IRUSR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'ONNX'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\mindsp161\\lib\\site-packages\\mindspore\\common\\api.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(self, file_name, graph_id)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \"\"\"\n\u001b[0;32m    768\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_expression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m         \u001b[0mexport_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AIR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch_info_for_quant_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexec_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mindspore\\ccsrc\\pipeline\\jit\\pipeline.cc:1720 ExportGraph] Only support export file in 'AIR' format with Ascend backend.\n\n# In file C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6188\\45943137.py(61)\n"
     ]
    }
   ],
   "source": [
    "export(lenet000,Tensor(input000),file_name='lenet5_1-10_2',file_format=\"AIR\")\n",
    "# 导出以 .air 结尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(lenet000,Tensor(input000),file_name='outModel/lenet5_1-10_2',file_format=\"ONNX\")\n",
    "# 导出的文件以 .oonx 结尾"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e203e762bfffed2ed95f356b7746f45c38e9111b20c3e823e88c83c42bbc40aa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('mindsp161')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
